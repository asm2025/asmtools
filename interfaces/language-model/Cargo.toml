[package]
name = "kalosm-language-model"
version = "0.2.1"
edition = "2021"
description = "A common interface for language models/transformers "
license = "MIT/Apache-2.0"
repository = "https://github.com/floneum/floneum"
authors = ["Evan Almloff"]
keywords = ["ai", "llm", "llama", "mistral", "nlp"]

[dependencies]
futures-util = "0"
llm-samplers = { workspace = true }
log = "0"
rand = "0"
reqwest = { version = "0", features = ["stream", "json"] }
tokio = { version = "1", features = ["full"] }
slab = { version = "0", features = ["serde"] }
serde = { version = "1", features = ["derive"] }
once_cell = "1"
url = "2"
anyhow = "1"
tracing = "0"
num_cpus = "1"
async-openai = "0"
async-trait = "0"
serde_json = "1"
tempfile = "3"
candle-core.workspace = true
tokio-util = { version = "0", features = ["rt"] }
pin-project = "1"
itertools = "0"
tokenizers = { version = "0" }
rustc-hash = "1"
kalosm-sample = { workspace = true }
kalosm-common.workspace = true
kalosm-streams.workspace = true
# Required for LLM
llm = { git = "https://github.com/rustformers/llm", optional = true }
spinoff = "0"
bytesize = "1"
minijinja = "1"

[features]
llamacpp = ["llm", "kalosm-sample/llamacpp"]
metal = ["llm?/metal"]
cublas = ["llm?/cublas"]
