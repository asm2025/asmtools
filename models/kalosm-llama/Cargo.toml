[package]
name = "kalosm-llama"
version = "0.2.2"
edition = "2021"
description = "A simple interface for Llama models "
license = "MIT/Apache-2.0"
repository = "https://github.com/floneum/floneum"
authors = ["Evan Almloff"]
keywords = ["llm", "llama", "mistral", "agents", "nlp"]

[dependencies]
candle-core = { workspace = true }
candle-nn = { workspace = true }
candle-transformers = { workspace = true }
tokenizers = { version = "0" }

accelerate-src = { version = "0", optional = true }
intel-mkl-src = { version = "0", features = ["mkl-static-lp64-iomp"], optional = true }
cudarc = { version = "0", features = ["f16"], optional = true }
half = { version = "2" }

anyhow = "1"
tracing = "0"
rand = "0"
tokio = { version = "1", features = ["full"] }
async-trait = "0"
once_cell = "1"
rayon = { version = "1", optional = true }
llm-samplers = { workspace = true }
kalosm-sample = { workspace = true }
kalosm-language-model = { workspace = true }
kalosm-streams = { workspace = true }
kalosm-common = { workspace = true }

[dev-dependencies]
tracing-subscriber = "0"
criterion = "0"
kalosm = { path = "../../interfaces/kalosm" }

[features]
default = []
accelerate = ["dep:accelerate-src", "candle-core/accelerate", "candle-nn/accelerate", "candle-transformers/accelerate"]
cuda = ["candle-core/cuda", "candle-nn/cuda", "candle-transformers/cuda"]
cudnn = ["candle-core/cudnn"]
mkl = ["dep:intel-mkl-src", "candle-core/mkl", "candle-nn/mkl", "candle-transformers/mkl"]
nccl = ["cuda", "cudarc/nccl", "half/num-traits", "half/use-intrinsics", "half/rand_distr"]
metal = ["candle-core/metal", "candle-nn/metal", "candle-transformers/metal"]
rayon = ["dep:rayon"]

[[bench]]
name = "inferance"
harness = false
